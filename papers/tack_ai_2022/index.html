<!DOCTYPE html>
<html lang="">

<head><!-- Metadata, OpenGraph and Schema.org -->


    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Anaïs Tack | The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues</title>
    <meta name="author" content="Anaïs  Tack" />
    <meta name="description" content="" />

<!-- Embedded Metadata -->
<meta name="citation_author" content="Anaïs Tack"><meta name="citation_author" content="Chris Piech"><meta name="citation_date" content="2022">
<meta name="citation_title" content="The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues"><meta name="citation_inbook_title" content="Proceedings of the 15th International Conference on Educational Data Mining">
<meta name="citation_conference_title" content="The  15th International Conference on Educational Data Mining"><meta name="citation_abstract" content="How can we test whether state-of-the-art generative models, such as Blender and GPT-3, are good AI teachers, capable of replying to a student in an educational dialogue? Designing an AI teacher test is challenging: although evaluation methods are much-needed, there is no off-the-shelf solution to measuring pedagogical ability. This paper reports on a first attempt at an AI teacher test. We built a solution around the insight that you can run conversational agents in parallel to human teachers in real-world dialogues, simulate how different agents would respond to a student, and compare these counterpart responses in terms of three abilities: speak like a teacher, understand a student, help a student. Our method builds on the reliability of comparative judgments in education and uses a probabilistic model and Bayesian sampling to infer estimates of pedagogical ability. We find that, even though conversational agents (Blender in particular) perform well on conversational uptake, they are quantifiably worse than real teachers on several pedagogical dimensions, especially with regard to helpfulness (Blender: ∆ ability = -0.75; GPT-3: ∆ ability = -0.93)."><meta name="citation_firstpage" content="522–529">
<meta name="citation_lastpage" content=""><meta name="citation_pdf_url" content="/assets/pdf/tack_ai_2022.pdf">

<!-- Bootstrap -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_light" />

<!-- Styles -->

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://anaistack.github.io/papers/tack_ai_2022/">

<!-- Dark Mode -->

</head>

<body class="fixed-top-nav sticky-bottom-footer font-serif">

<header><nav class="navbar navbar-expand-lg navbar-light bg-light navbar-expand-sm fixed-top font-sans-serif spaced-lower-small-caps">
  <div class="container-fluid">
    <a class="navbar-brand title font-weight-bold" href="https://anaistack.github.io/">Anaïs Tack</a>

    <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbar-collapse"
      aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbar-collapse">
      <div class="navbar-nav flex-nowrap">
        
        <a class="nav-item nav-link " href="/publications/">
          Publications
        </a>
        
        <a class="nav-item nav-link " href="/projects/">
          Projects
        </a>
        
        <a class="nav-item nav-link " href="/tools/">
          Tools
        </a>
        
        <a class="nav-item nav-link " href="/contact">
          Contact
        </a>
        
      </div>
      <div class="navbar-brand social ml-auto">
        <a href="https://orcid.org/0000-0003-3086-8188" title="ORCID"><i class="ai ai-orcid fa-fw fa-xs"></i></a>
<a href="https://scholar.google.com/citations?user=dpcqhuIAAAAJ" title="Google Scholar"><i
        class="ai ai-google-scholar ai-fw ai-xs"></i></a>
<a href="https://github.com/anaistack" title="GitHub"><i class="fab fa-github fa-fw fa-xs"></i></a>
<a href="https://www.linkedin.com/in/anaïs-tack-95b869a1" title="LinkedIn"><i class="fab fa-linkedin fa-fw fa-xs"></i></a>

      </div>
    </div>

  </div>
</nav>
</header>

<div class="container container-md mt-5">


<h1>The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues</h1>

<hr>

<dl>
  <dt>Authors</dt>
  <dd>
<strong>Tack, Anaïs</strong>,&nbsp;& <a href="https://stanford.edu/~cpiech/">Piech, Chris</a>
</dd>
  <dt>In</dt>
  <dd>
  <em>Proceedings of the 15th International Conference on Educational Data Mining</em></dd>
  <dt>Pages</dt>
  <dd>522–529</dd><dt>Year</dt>
  <dd>2022</dd><dt>Abstract</dt>
  <dd>How can we test whether state-of-the-art generative models, such as Blender and GPT-3, are good AI teachers, capable of replying to a student in an educational dialogue? Designing an AI teacher test is challenging: although evaluation methods are much-needed, there is no off-the-shelf solution to measuring pedagogical ability. This paper reports on a first attempt at an AI teacher test. We built a solution around the insight that you can run conversational agents in parallel to human teachers in real-world dialogues, simulate how different agents would respond to a student, and compare these counterpart responses in terms of three abilities: speak like a teacher, understand a student, help a student. Our method builds on the reliability of comparative judgments in education and uses a probabilistic model and Bayesian sampling to infer estimates of pedagogical ability. We find that, even though conversational agents (Blender in particular) perform well on conversational uptake, they are quantifiably worse than real teachers on several pedagogical dimensions, especially with regard to helpfulness (Blender: ∆ ability = -0.75; GPT-3: ∆ ability = -0.93).</dd></dl>

      <!-- Links/Buttons -->
      <div class="links font-sans-serif">
        <a href="https://educationaldatamining.org/edm2022/proceedings/2022.EDM-short-papers.54/index.html" class="badge badge-dark" role="button">HTML</a>
        <a href="https://educationaldatamining.org/edm2022/proceedings/2022.EDM-short-papers.54/2022.EDM-short-papers.54.pdf" class="badge badge-dark">PDF</a>
        <a href="/assets/pdf/EDM-KnowledgeAndPerformanceModeling3-168-AnaisTack.pdf" class="badge badge-light" role="button">Slides</a>
    </div>

</div><footer class="border-top sticky-bottom font-sans-serif txt-center">
  <div class="container py-3 text-center">
    <span class="text-muted">
      &copy; 2024 Anaïs Tack
      &middot; Last updated on Apr 22, 24.
      
    </span>
  </div>
</footer>

<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>


</body>
</html>